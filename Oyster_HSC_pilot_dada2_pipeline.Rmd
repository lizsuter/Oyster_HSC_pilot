---
title: "Oyster Horseshoe Crab Pilot Project from ENV4800"
author: "Liz Suter"
date: "Jan 13 2020"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
<br>


### Install and load packages

No need to do any installations in VICE app. Packages should come pre-installed. But need to run libraries:

```{r}
library(dada2)
library(DECIPHER)
```

## Upload fastq files and trim primers

### Get sample names- run in Terminal
```{bash}
cd Raw_data
ls *R1_001.fastq.gz | cut -f 1-2 -d "_" > ../samples
```


### Take a look at the untrimmed reads
```{r}
## import sample names as R variable
samples <- scan("samples", what="character")

# make variable holding the file names of all the forward read fastq files. These are in a subdirectory, so I am also adding the name of the sub directory to the file name
forward_reads <- paste0("Raw_data/", samples, "_L001_R1_001.fastq.gz")
# and one with the reverse
reverse_reads <- paste0("Raw_data/", samples, "_L001_R2_001.fastq.gz")

# And plot using a tool from dada2 (checking only 5 samples for plotting purposes)
plotQualityProfile(forward_reads[1:5])
plotQualityProfile(reverse_reads[1:5])
```



From the above you can see the reads are 250bp and the quality is OK, with the R reads being poorer than the F reads. 

### Removing primers using cutadapt- run in Terminal
Sequencing facility told me that the way that they sequence in a way that primers do not get sequenced. Still, check for and remove possible primers


Primers from paper Caporaso et al. 2012:
515f 5'-GTGCCAGCMGCCGCGGTAA-3' [rev comp: TTACCGCGGCKGCTGGCAC]
806r 5'-GGACTACHVGGGTWTCTAAT-3' [rev comp: ATTAGAWACCCBDGTAGTCC]

Cut primers with `-b` and `-B` options, which looks for it at both 5' and 3' ends. Set the min size to be about 10% smaller than the amplicon length (225). Don't use the `discard--untrimmed` option, since I don't want to throw out untrimmed sequences (since these have been removed already by the sequencing setup). 


```{bash}
cd ..
mkdir trimmed_fastq
cd Raw_data

# Run in loop

for sample in $(cat ../samples)
do
    echo "On sample: $sample"
cutadapt -b ATTAGAWACCCBDGTAGTCC \
-b GTGCCAGCMGCCGCGGTAA \
-B TTACCGCGGCKGCTGGCAC \
-B GGACTACHVGGGTWTCTAAT \
-m 225 \
-o ../trimmed_fastq/${sample}_L001_R1_001_trimmed.fastq.gz -p ../trimmed_fastq/${sample}_L001_R2_001_trimmed.fastq.gz \
${sample}_L001_R1_001.fastq.gz ${sample}_L001_R2_001.fastq.gz \
>> ../trimmed_fastq/cutadapt_primer_trimming_stats.txt 2>&1
done

```

Check output stats
```{bash}
paste ../samples <(grep "passing" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")") <(grep "filtered" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")")
```

Retained ~99.9-100% mostly because I didn't use ```discard-untrimmed``` option.


## DADA2 pipeline

Take a look at the trimmed reads.

```{r}
forward_reads_trimmed <- paste0("trimmed_fastq/", samples, "_L001_R1_001_trimmed.fastq.gz")
reverse_reads_trimmed <- paste0("trimmed_fastq/", samples, "_L001_R2_001_trimmed.fastq.gz")

# And plot 
plotQualityProfile(forward_reads_trimmed[1:5])
plotQualityProfile(reverse_reads_trimmed[1:5])
```

Comparing the above to the pre-trimmed reads, they look very similar because only very few had primers. 

### Quality Trimming
Make a directory for filtered reads
```{bash}
cd ..
mkdir filtered_fastq
```

Make variables containing the file names for the new filtered forward and reverse reads that we will make
```{r}
filtered_forward_reads <- paste0("filtered_fastq/",samples, "_L001_R1_001_filtered.fastq.gz")
filtered_reverse_reads <- paste0("filtered_fastq/",samples, "_L001_R2_001_filtered.fastq.gz")
```


Based on how the quality plots look, determine how much to cut from each side based on where the quality drops below ~30. Trim the F reads at 225. Trim the R reads to 175. Also I want to run this step to trim out the low-quality individual reads (set maxEE to 1 for both F and R reads). The rm.phix = TRUE option removes any leftover PhiX genomic DNA (that gets added as a standard during sequencing). Pick a min length ~shorter than the min trimmed length (in this case 200 for R reads). I also set truncQ to truncate any read that has a quality score less than 2. Multithreading for this function does not work well (even according to documentation) so needed to skip that. Takes a bit of time to run.
```{r}
filtered_out <- filterAndTrim(forward_reads_trimmed, filtered_forward_reads,
                reverse_reads_trimmed, filtered_reverse_reads, maxEE=c(2,2),
                rm.phix=TRUE, minLen=150, truncLen=c(225,175), truncQ = 2, maxN=0)
```

Check out the quality profiles again.
```{r}
filtered_out

plotQualityProfile(filtered_forward_reads[1:5])
plotQualityProfile(filtered_reverse_reads[1:5])
```
These look a little better but I am still concerned about some of those dips in the R reads

Save workspace up to this point.
```{r}
#dir.create("backups")
save.image(file = "backups/upto_filterfastq.RData")
```



### Error profiling
Next, DADA2 tries to learn the error signature of our dataset. This step takes a while
```{r}
err_forward_reads <- learnErrors(filtered_forward_reads, multithread=TRUE)
err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread=TRUE)
```



Plot the error profiles
```{r}
plotErrors(err_forward_reads, nominalQ=TRUE)
plotErrors(err_reverse_reads, nominalQ=TRUE)
```

The creators of DADA2 describe this [here](https://benjjneb.github.io/dada2/tutorial.html#learn-the-error-rates). The profiles are the error rates for each possible transition in the read (A->C, A->G, etc). Generally in the above plots, you want to see that the black dots (observed error rates for each quality score) match well with the black lines (the estimated error rate). The red line is what is expected based on the quality score.

Backup again since this step above takes awhile
Save workspace up to this point.
```{r}
save.image(file = "backups/upto_errorprofile.RData")
```


### Inferring ASVs
Use the dada command to infer ASVs. We are going to use the pooling option "psuedo" which is described [here](https://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling). This step also takes awhile
```{r}
dada_forward <- dada(filtered_forward_reads, err=err_forward_reads, pool="pseudo", multithread=TRUE) 
dada_reverse <- dada(filtered_reverse_reads, err=err_reverse_reads, pool="pseudo", multithread=TRUE)
```

Backup again 

```{r}
save.image(file = "backups/upto_inferasv.RData")
```


### Merge inferred reads 
Dada2 will merge reads wherever the overlap is identical between the F and R reads. I trimmed the F reads to 225 and R reads to 175 (from an initial read length of 250). The full amplicon size (based on primers) should be 806-515=290bp. So the F read should be sequenced from ~ 515 to 740 (515+225) and the R reads should be sequenced from ~ position 806 to 631 (806-160). This leaves a region of overlap between 631 and 740, or 109bp total. Since this is an estimate (and is also based on positions in E coli and may not be exactly true for all organisms), we can leave a little wiggle room and set the minimum overlap to 50bp. Also set trimOverhang to true, which makes sure that a read doesn't go past its opposite primer (which probably wouldn't happpen any way due to trimming).
```{r}
merged_amplicons <- mergePairs(dada_forward, filtered_forward_reads, dada_reverse, filtered_reverse_reads, trimOverhang=TRUE, minOverlap=50, verbose = TRUE)
```


```{r}
names(merged_amplicons)
# Initially these names have the full name with `fastq.gz` in the name. Change to just sample name
names(merged_amplicons) <- samples

# Check some other things
length(merged_amplicons) # 40 elements in this list, one for each of our samples
class(merged_amplicons$A1_S15) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe
names(merged_amplicons$A1_S15) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"
```

Back up again
```{r}
save.image(file = "backups/upto_merge.RData")
```


### Creating a sequence table
```{r}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab) # matrix
dim(seqtab) # 40 samples, 2482 unique ASVs
```

### Removing chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose=TRUE) 

# Identified 43 bimeras out of 2482 input sequences.

# though we a lot of unique sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab) 
# 0.9802759
# good, we barely lost any in terms of abundance. That means the chimeras were very low abundance "ASVs"
```

Backup again since this step above takes awhile
Save workspace up to this point.
```{r}
save.image(file = "backups/upto_chimera.RData")
```


# Summary of read counts through the pipeline
```{r}
# set a little function
getN <- function(x) sum(getUniques(x))

# making a little table
summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1],
                          filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN),
                          dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab
```
In the end, we retained abot 40-85% of input reads after the filtering steps. Not bad

### Assigning taxonomy using DECIPHER 

**Note:** DADA2 typically uses RDP's kmer-based method using SILVA for its classification database. Here we will use idTAXA as part of the DECIPHER package to see how results compare to the QIIME2 DADA2 workflow.

Download the DECIPHER training set as an RData file. The most up-to-date one is v138 (which is also newer than the one available in QIIME)

```{bash}
curl -o SILVA_SSU_r138_2019.RData http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData
```


Now assign taxonomy. This step also takes awhile (>4-8 hours) 
```{r}
load("SILVA_SSU_r138_2019.RData") 

# Create a DNAStringSet from the ASVs
dna <- DNAStringSet(getSequences(seqtab.nochim))

# Run Classification Step
ids <- IdTaxa(test=dna, trainingSet=trainingSet, strand="both", processors=NULL, verbose=TRUE) # use all processors
```

### Extract the files 

```{r}
#dir.create("dada2_results")

#giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")
for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "dada2_results/ASVs.fa")

# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "dada2_results/ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

# Taxonomy table
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species")
asv_tax <- t(sapply(ids, function(x) {
  m <- match(ranks, x$rank)
  taxa <- x$taxon[m]
  taxa[startsWith(taxa, "unclassified_")] <- NA
  taxa
}))
colnames(asv_tax) <- ranks
rownames(asv_tax) <- gsub(pattern=">", replacement="", x=asv_headers)

write.table(asv_tax, "dada2_results/ASVs_taxonomy.tsv", sep = "\t", quote=F, col.names=NA)
```

Backup 

```{r}
save.image(file = "backups/final.RData")
```


## Plotting Abundances

